- script.py: kaggle上一哥们的kernel（stack各种其他方法），效果还行
- script_tf_lgb_v1001.py: 用AutoScaler和DataFrameImputer预处理
- script_tf_lgb_v1002.py: 用AutoScaler和DataFrameImputer预处理，再接nn生成的特征
- script_tf_lgb_v1003.py: 相比v1002加了正则项，生成能用于stacking的训练集
- script_tf_lgb_v1004.py: 使用kueipo的特征，不训练，随机出来大量特征，让lgb选, 输出evals_result观察每个cv的结果
- script_merge_test_out.py: 融合其他分类器的结果，平均输出
- script_xgb_aharless.py: cv:0.286,lb:0.284,ensemble:0.285, https://www.kaggle.com/aharless/xgboost-cv-lb-284
- script_zusmani.py: lb:0.267, https://www.kaggle.com/zusmani/lgb-esemble-xgb-be-in-top-100-with-lb-0-285
- script_kueipo.py: 对count数>2, <7的特征做one_hot; cv:0.283, self-lb:, ensemble-lb:0.285,
- script_kueipo_v2.py: 把n折全用上，(cv:0.284061, self-lb:0.282, ensemble-lb:)(没调参)，(0.28405)(用kueipo的参数)
- script_tf_lgb_v1003.py: （只用raw或+nn）用原始的所有特征, 对lgb进行了调参
- script_v2.py: script.py去掉tensorflow，cv的时候避免overfit操作，（没用dnn、biggp、kinetics）
- stack_with_files.py: 根据其他模型生成train.csv和test.csv，而进行stacking
- script_pure_mlp_v2001.py: 纯nn模型
- script_pure_mlp_v2002.py: 半监督学习auto-encoder，加一组并行的nn处理test数据，与train共享浅层layer。
- script_pure_mlp_v2003.py: 半监督学习auto-encoder，好像和v2002差不多
- script_pure_mlp_v2004.py: 基于v2001，使用camnugent的fe, AutoScaler比StandardScaler的cv好0.002, cv:0.263(0.265), 0.2643
- script_pure_mlp_v2005.py: 在v2004的基础上，使用aquatic的FE。**cv:0.2698,cv-var:3.7e-6**;  *cv: 0.2679,0.2673,0.2684*(这组结果预测时没有正确使用dropout和noiselayer)
- script_pure_mlp_v2006.py: 写一个函数专门做大数据集的predict(通过切分为minibatch实现). **cv:0.2726,var:1.5e-5,lb:0.274**
- script_pure_mlp_v2007.py: 只用有标记的dataset加上GAN试试, 效果很差不知道是不是G和D同时训练导致的，开v2009试试
- script_pure_mlp_v2008.py: 在v2006的基础上改正了batch_normalization不同阶段的使用
- script_pure_mlp_v2009.py: 学习gitlimlab的SSGAN，分阶段分别训练G和D试试, 有tensorboard; 在此程序文件的最后面，有with/without GAN的性能对比
- script_pure_mlp_v2010.py: 在v2009的基础上，去掉一些注释，自动创建跟文件名相关的路径，使用continuation method加退火(目前有bug啊20171206！可能是训练没收敛（我在有道笔记中记录了相关记录）)
- script_pure_mlp_v2011.py: 在v2010的基础上，使用RMSPropOptimizer，调低了总迭代次数和学习率

- script_reproduce_kaggle_1st_dae00x1.py: 使用HDFDataSet把特征工程完的数据存到一个h5中
- script_reproduce_kaggle_1st_dae00x2.py: 在dae00x1和dae015的基础上，使用RankGauss归一化非binary的特征，并存成tfrecord
- script_reproduce_kaggle_1st_dae00x2b.py: 在dae00x2的基础上，改了新的路径base_path和data_path
- script_reproduce_kaggle_1st_dae00x3.py: 在dae00x2的基础上，将整个训练集分成（训练集，验证集）存成2个tfrecord文件
- script_reproduce_kaggle_1st_dae00x4.py: 在dae00x3的基础上改了新的路径base_path和data_path
- script_reproduce_kaggle_1st_dae00x5.py: 在dae00x2b的基础上修改了rank_gauss（使用了scipy.stats.rankdata的dense模式）
- script_reproduce_kaggle_1st_dae00x6.py: 生成(train,valid)，在dae00x4的基础上使用新的rank_gauss(和dae00x5一样)

- script_reproduce_kaggle_1st_dae001.py: 希望复现本次比赛第一的方案，首先要复现出特征工程的内容; 复现了mjahrer的#1模型
- script_reproduce_kaggle_1st_dae002.py: 使用hyperopt调sklearn伪造数据的lightgbm参数
- script_reproduce_kaggle_1st_dae003.py: 使用dae00x1生成的数据，在the3机器上运行了hyperopt
- script_reproduce_kaggle_1st_dae004.py: 使用dae00x1生成的数据，修改hyperopt的space，在the5机器上运行; 在最后的cell使用best model做预测
- script_reproduce_kaggle_1st_dae005.py: 希望实现mjahrer的#2的dae模型; 加了scaled_feature的处理程序（其实这个应该在dae00x1中做，在添加完hdf后我已经给注释掉了）
- script_reproduce_kaggle_1st_dae006.py: fix了batch generator，fix了bn的phase_train问题; 有错误啊最后一层不应该用dropout
- script_reproduce_kaggle_1st_dae007.py: 将dae006生成的dae_hidden_feature喂给lgb
- script_reproduce_kaggle_1st_dae008.py: 将dae006生成的数据给nn, 太耗内存
- script_reproduce_kaggle_1st_dae009.py: 使用dae00x1生成的特征，dae008太耗内存，重写batch_generator和balance_train_data, 用了Adam
- script_reproduce_kaggle_1st_dae010.py: 使用PPMoney的DataQueue
- script_reproduce_kaggle_1st_dae011.py: 不把所有X数据读入内存，每次根据索引从h5中读batch, 又慢又费内存
- script_reproduce_kaggle_1st_dae012.py: 根据dae009重新写, 使用DataQueue的单线程
- script_reproduce_kaggle_1st_dae013.py: 训练有监督nn，不使用DataQueue和batch_idx_gen, 跑起来需要100G内存，用了RMSProp
- script_reproduce_kaggle_1st_dae014.py: 基于dae006，最后一层不加dropout和activation
- script_reproduce_kaggle_1st_dae015.py: 从HDFDataSet读数据，存成tfrecord的格式
- script_reproduce_kaggle_1st_dae016.py: 使用dae015生成的tfrecord训练dae特征（重复dae014修改了optimizer和learning_rate）
- script_reproduce_kaggle_1st_dae017.py: 使用dae00x2生成的tfrecord训练dae特征, 其他跟dae016一样
- script_reproduce_kaggle_1st_dae018.py: 载入并复用dae017的model，有监督nn参考了一下dae013，optimizer参考了dae017
- script_reproduce_kaggle_1st_dae019.py: 使用dae018生成的模型预测dae00x2生成的测试集，生成提交到kaggle的文件。会根据dae018不同的名字改里面model_name的名字。(dae017,dae018,dae019都存在一个很蠢的问题，每次都要重新拼接graph、重新处理tfrecord的iterator来兼容新的graph，应该形成一个统一的方案，包括：autoencoder、training、cv、predict)
- script_reproduce_kaggle_1st_dae020.py: 在dae018的基础上，使用dae00x3生成的（训练集，验证集）+ feedable iterator
- script_reproduce_kaggle_1st_dae021.py: 使用dae00x5的特征，其他跟dae017一样
- script_reproduce_kaggle_1st_dae022.py: 前置无监督nn使用dae021，使用dae00x6的(train, valid)特征；bn层一定要加；*0.25*
- script_reproduce_kaggle_1st_dae023.py: 相比dae021增加了bn层，epoch数调到500（快速试一下，稳定后调到1000）
- script_reproduce_kaggle_1st_dae024.py: 跟dae022差不多，前置无监督nn使用dae023，使用dae00x6的(train, valid)特征；*0.22*
- script_reproduce_kaggle_1st_dae025.py: 和dae017一样，修改了路径名，dae00x2生成的特征
- script_reproduce_kaggle_1st_dae026.py: 跟dae024差不多，前置无监督nn使用dae025，使用dae00x3的(train, valid)特征 *0.22*
- script_reproduce_kaggle_1st_dae028.py: 仿照dae024，前置无监督nn使用dae021，使用dae020b的参数 *0.22*
- script_reproduce_kaggle_1st_dae029.py: 基于dae020b1(修改了lr和l2_scale)，前置nn使用dae017b
*lr:5e-4,l2_scale:0.05过拟合非常快(如果比不加l2_scale得分0.272几乎到300轮最后才收敛)*
*lr:1e-4,l2:0.05, gini:0.263*
- script_reproduce_kaggle_1st_dae030.py: 相比dae029多了input_keep_rate *0.260*
- script_reproduce_kaggle_1st_dae031.py: 相比dae020b1，修改了decay_steps为steps_per_epoch
- script_reproduce_kaggle_1st_dae032.py: 使用的应该是swapnoise，其他的跟disscusion里说的应该是一样
- script_reproduce_kaggle_1st_dae033.py: 前置nn用的dae032，其他的跟dae026差不多

**看看(dae025,dae026)相比(dae017b,dae020b1)是什么根本的差异导致了结果差很多呢？超参数的细微差别不会导致这么夸张**

- script_tune_camnugent_dnn.py: script.py脚本里camnugent的tensorflow程序段
- script_tune_camnugent_dnn_v2.py: 希望使用tf.learn做cv，但是写的很乱
- script_tune_camnugent_dnn_v3.py: 基于script_tune_camnugent_dnn.py, 简单测试一下eval的效果，证明camnugent的fe对nn有效
- script_ogrellier_xgb.py: ogrellier的kernel（这哥们好像用boruta + lightgbm选的特征，tilii7这哥们用boruta + RF选的特征），有target_encode，cv:
- script_aquatic_keras_nn.py:
